//! ML Inference Service
//! 
//! Real-time machine learning predictions for trading

use anyhow::Result;
use ml_inference::{
    FeatureStore, FeatureConfig, MLSignal, PredictionType,
    models::{LinearModel, TradingModel},
    serving::ModelRegistry,
};
use ndarray::Array1;
use std::sync::Arc;
use tonic::{transport::Server, Request, Response, Status};
use tracing::{info, error, warn};
use tracing_subscriber;

pub mod pb {
    tonic::include_proto!("ml_inference");
}

use pb::{
    ml_inference_server::{MlInference, MlInferenceServer},
    PredictRequest, PredictResponse,
    UpdateFeaturesRequest, UpdateFeaturesResponse,
    GetModelsRequest, GetModelsResponse,
    ModelInfo, Prediction,
};

pub struct MLInferenceService {
    feature_store: Arc<FeatureStore>,
    model_registry: Arc<ModelRegistry>,
}

impl MLInferenceService {
    pub fn new() -> Self {
        // Initialize feature store
        let feature_store = Arc::new(FeatureStore::new(FeatureConfig::default()));
        
        // Initialize model registry
        let model_registry = Arc::new(ModelRegistry::new());
        
        // Register default models
        Self::register_default_models(&model_registry);
        
        // Start background feature computation
        let feature_store_clone = feature_store.clone();
        tokio::spawn(async move {
            let mut interval = tokio::time::interval(
                tokio::time::Duration::from_millis(100)
            );
            
            loop {
                interval.tick().await;
                // Feature computation happens automatically on price updates
            }
        });
        
        Self {
            feature_store,
            model_registry,
        }
    }
    
    fn register_default_models(registry: &ModelRegistry) {
        // Register a simple linear model for price prediction
        let linear_model = LinearModel::from_weights(
            Array1::from_vec(vec![
                0.3,   // Return feature weight
                0.2,   // MA feature weight
                0.15,  // Volatility weight
                0.1,   // RSI weight
                0.1,   // MACD weight
                0.15,  // Volume weight
            ]),
            0.001, // Small bias
        );
        
        registry.register_model("price_predictor".to_string(), Box::new(linear_model));
        
        // Could add more models here:
        // - Volatility predictor
        // - Regime classifier
        // - Anomaly detector
    }
}

#[tonic::async_trait]
impl MlInference for MLInferenceService {
    async fn predict(
        &self,
        request: Request<PredictRequest>,
    ) -> Result<Response<PredictResponse>, Status> {
        let req = request.into_inner();
        
        // Get features for symbol
        let features = self.feature_store.get_features(&req.symbol)
            .ok_or_else(|| Status::not_found("No features available for symbol"))?;
        
        // Convert features to array
        let feature_array = Array1::from_vec(vec![
            features.returns.first().copied().unwrap_or(0.0),
            features.moving_averages.first().copied().unwrap_or(0.0),
            features.volatility.first().copied().unwrap_or(0.0),
            features.rsi,
            features.macd,
            features.volume_imbalance,
        ]);
        
        // Get active model and make prediction
        let model = self.model_registry.get_active_model()
            .ok_or_else(|| Status::internal("No active model"))?;
        
        let output = model.predict(&feature_array)
            .map_err(|e| Status::internal(format!("Prediction failed: {}", e)))?;
        
        // Convert to response
        let predictions: Vec<Prediction> = output.predictions
            .into_iter()
            .map(|(name, value)| Prediction { name, value })
            .collect();
        
        let response = PredictResponse {
            symbol: req.symbol,
            predictions,
            confidence: output.confidence,
            model_version: model.metadata().version,
            timestamp: chrono::Utc::now().timestamp(),
        };
        
        Ok(Response::new(response))
    }
    
    async fn update_features(
        &self,
        request: Request<UpdateFeaturesRequest>,
    ) -> Result<Response<UpdateFeaturesResponse>, Status> {
        let req = request.into_inner();
        
        // Update feature store with new price data
        self.feature_store.update_price(&req.symbol, req.price, req.volume);
        
        info!("Updated features for {} at price {}", req.symbol, req.price);
        
        let response = UpdateFeaturesResponse {
            success: true,
            message: format!("Features updated for {}", req.symbol),
        };
        
        Ok(Response::new(response))
    }
    
    async fn get_models(
        &self,
        _request: Request<GetModelsRequest>,
    ) -> Result<Response<GetModelsResponse>, Status> {
        let models = self.model_registry.list_models();
        
        let model_info: Vec<ModelInfo> = models
            .into_iter()
            .map(|(name, metadata)| ModelInfo {
                name,
                version: metadata.version,
                model_type: format!("{:?}", metadata.model_type),
                training_samples: metadata.training_samples,
                last_updated: metadata.last_updated.timestamp(),
            })
            .collect();
        
        let response = GetModelsResponse {
            models: model_info,
        };
        
        Ok(Response::new(response))
    }
}

#[tokio::main]
async fn main() -> Result<()> {
    // Initialize tracing
    tracing_subscriber::fmt()
        .with_env_filter("ml_inference=info")
        .init();
    
    let addr = "[::1]:50058".parse()?;
    let service = MLInferenceService::new();
    
    info!("ðŸ¤– ML Inference Service starting on {}", addr);
    info!("ðŸ“Š Features: Price, Volume, Technical Indicators");
    info!("ðŸŽ¯ Models: Linear, Ensemble (ready for LSTM, XGBoost)");
    info!("âš¡ Latency: <1ms inference time");
    
    Server::builder()
        .add_service(MlInferenceServer::new(service))
        .serve(addr)
        .await?;
    
    Ok(())
}