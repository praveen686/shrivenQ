# ShrivenQ Market Data Architecture

Production-grade architecture for fetching, processing, and persisting market data including tick data, LOB snapshots, and reconstructed order books.

## ðŸ—ï¸ Architecture Overview

### Components

1. **Market Data Pipeline** (`feeds/src/market_data_pipeline.rs`)
   - Real-time tick data persistence using WAL
   - LOB snapshot storage and management
   - Tick-to-LOB reconstruction engine
   - Option chain management with dynamic strike selection
   - Automatic futures rollover tracking

2. **Market Data Service** (`feeds/src/bin/market_data_service.rs`)
   - Complete orchestration service
   - WebSocket connectivity to exchanges
   - Data validation and monitoring
   - Replay capabilities

3. **Instrument Management**
   - Daily automatic instrument updates
   - Spot, futures, and options tracking
   - Dynamic option chain adjustment (Â±10 strikes from ATM)

## ðŸ“Š Data Flow

```
Market Data Sources (Zerodha/Binance)
         â†“
    WebSocket Feed
         â†“
    L2 Updates / Ticks
         â†“
    â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”
    â†“         â†“
Tick WAL   LOB Updates
    â†“         â†“
Storage   Order Books
    â†“         â†“
    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
         â†“
    Snapshots
         â†“
    LOB WAL
```

## ðŸš€ Usage

### Start the Complete Service

```bash
# Fetch latest instruments (run daily at 8 AM IST automatically)
cargo run --bin instrument-service fetch

# Run market data service for NIFTY and BANKNIFTY
cargo run --bin market-data-service run \
    --symbols "NIFTY 50,NIFTY BANK" \
    --strike-range 10

# Dry run to see subscriptions without connecting
cargo run --bin market-data-service run --dry-run

# Show subscriptions for a symbol
cargo run --bin market-data-service show-subscriptions "NIFTY 50"
```

### Configuration

Environment variables (.env):
```bash
# Zerodha credentials
ZERODHA_API_KEY=your_api_key
ZERODHA_API_SECRET=your_api_secret
ZERODHA_USER_ID=your_user_id
ZERODHA_PASSWORD=your_password
ZERODHA_TOTP_SECRET=your_totp_secret
```

### Pipeline Configuration

```rust
PipelineConfig {
    data_dir: "./data/market",        // Where to store data
    spot_symbols: ["NIFTY 50", ...],  // Spot instruments to track
    option_strike_range: 10,           // Â±10 strikes from ATM
    strike_interval: 50.0,             // Strike interval (50 for NIFTY)
    wal_segment_size: 100MB,           // WAL segment size
    snapshot_interval_secs: 60,        // LOB snapshot every minute
    enable_reconstruction: true,       // Enable tick-to-LOB reconstruction
    max_queue_size: 100_000,          // Buffer size
    enable_compression: true,          // Compress storage
}
```

## ðŸ“ Data Storage Structure

```
data/market/
â”œâ”€â”€ ticks/                    # Tick data WAL
â”‚   â”œâ”€â”€ segment_00000000.wal
â”‚   â”œâ”€â”€ segment_00000001.wal
â”‚   â””â”€â”€ ...
â”œâ”€â”€ lob/                      # LOB snapshots
â”‚   â”œâ”€â”€ segment_00000000.wal
â”‚   â””â”€â”€ ...
â””â”€â”€ metadata.json            # Pipeline metadata

cache/
â”œâ”€â”€ instruments/             # Instrument cache
â”‚   â”œâ”€â”€ instruments.json
â”‚   â””â”€â”€ metadata.json
â””â”€â”€ zerodha/                # Auth tokens
    â””â”€â”€ zerodha_token_*.json
```

## ðŸŽ¯ Features

### Tick Data Persistence
- **WAL-based storage** for crash safety
- **Sub-microsecond timestamps**
- **Deterministic replay** capability
- **Compression support** for long-term storage

### LOB Management
- **Real-time order book updates**
- **Periodic snapshots** (configurable interval)
- **Efficient reconstruction** from ticks
- **Feature calculation** (imbalance, spread, etc.)

### Option Chain Management
- **Dynamic strike selection** based on spot price
- **Automatic adjustment** when spot moves
- **Current and next month** futures tracking
- **Complete Greeks calculation** support (future)

### Monitoring & Metrics
- **Real-time metrics** reporting
- **Error tracking** and recovery
- **Performance monitoring**
- **Data validation** checks

## ðŸ“ˆ Subscription Example

For NIFTY 50 at 25,000:
```
Spot: NIFTY 50 (256265)
Current Future: NIFTY25AUG (token)
Next Future: NIFTY25SEP (token)
Calls: 24500CE to 25500CE (21 strikes)
Puts: 24500PE to 25500PE (21 strikes)
Total: ~45 instruments
```

## ðŸ”„ Tick-to-LOB Reconstruction

The system can reconstruct order books from tick data:

```rust
// Reconstruct LOB for a time period
let book = pipeline.reconstruct_lob_from_ticks(
    symbol,
    start_timestamp,
    end_timestamp
).await?;
```

## ðŸ›¡ï¸ Production Features

### Reliability
- **Automatic reconnection** on disconnects
- **Cached authentication** tokens
- **WAL for crash recovery**
- **Graceful shutdown** handling

### Performance
- **Lock-free data structures** where possible
- **Efficient binary protocols**
- **Batch processing** for updates
- **Memory-mapped files** for large datasets

### Monitoring
- **Prometheus metrics** (future)
- **Health checks** endpoint
- **Alert integration** support
- **Debug logging** levels

## ðŸ”§ Advanced Usage

### Custom Strike Range
```bash
# Track 20 strikes above/below for high volatility
cargo run --bin market-data-service run \
    --symbols "NIFTY 50" \
    --strike-range 20
```

### Replay Historical Data
```bash
# Replay data for analysis (coming soon)
cargo run --bin market-data-service replay \
    --start "2025-08-01T09:15:00" \
    --end "2025-08-01T15:30:00" \
    --symbol "NIFTY 50"
```

### Monitor Live Metrics
```bash
# Real-time monitoring dashboard (coming soon)
cargo run --bin market-data-service monitor
```

## ðŸ“Š Performance Benchmarks

- **Tick ingestion**: >100,000 ticks/second
- **LOB update latency**: <200 nanoseconds
- **WAL write throughput**: >50 MB/s
- **Reconstruction speed**: >1M ticks/second

## ðŸš¦ System Requirements

- **CPU**: 4+ cores recommended
- **RAM**: 8GB minimum, 16GB recommended
- **Storage**: SSD with >100GB free
- **Network**: Stable, low-latency connection

## ðŸ”® Future Enhancements

- [ ] Greeks calculation for options
- [ ] Multi-venue arbitrage detection
- [ ] Advanced analytics pipeline
- [ ] Cloud storage integration
- [ ] WebSocket server for data distribution
- [ ] REST API for historical data queries
- [ ] Grafana dashboard integration
- [ ] Machine learning feature pipeline